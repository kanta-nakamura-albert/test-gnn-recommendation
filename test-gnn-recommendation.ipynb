{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffaf18e2-4d7e-4963-b982-8e8e50d20ab9",
   "metadata": {},
   "source": [
    "# test-gnn-recommendation\n",
    "\n",
    "- 参考\n",
    "    - https://towardsdatascience.com/hands-on-graph-neural-networks-with-pytorch-pytorch-geometric-359487e221a8\n",
    "    - https://medium.com/arangodb/integrate-arangodb-with-pytorch-geometric-to-build-recommendation-systems-dd69db688465\n",
    "- サンプルデータ: [The Movies Dataset](https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset)\n",
    "    - `credits.csv`\n",
    "    - `keywords.csv`\n",
    "    - `links.csv`\n",
    "    - `links_small.csv`\n",
    "    - `movies_metadata.csv`\n",
    "    - `ratings.csv`\n",
    "    - `ratings_small.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f6630d2-c033-4a64-93e4-2978a66cb9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "from torch_geometric.transforms import RandomLinkSplit, ToUndirected\n",
    "\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "917cdde6-ceae-40cb-83a8-29808b055349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "LINKS_PATH = \"./data/links_small.csv\"\n",
    "METADATA_PATH = \"./data/movies_metadata.csv\"\n",
    "RATINGS_PATH = \"./data/ratings_small.csv\"\n",
    "\n",
    "df_links = pd.read_csv(LINKS_PATH, dtype=str)\n",
    "df_metadata = pd.read_csv(METADATA_PATH, dtype=str)\n",
    "df_ratings = pd.read_csv(RATINGS_PATH, dtype=str)\n",
    "\n",
    "df_metadata = df_metadata.drop([19730, 29503, 35587])\n",
    "df_metadata[\"title\"] = df_metadata[\"title\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3653475d-7a81-4b9c-9ef0-4ee716f47d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPUが使用可能な場合GPUを使用\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fafa647-f710-4316-885b-9dcc9f3e6d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_mappings(df, id_col):\n",
    "    mapping = {index: i for i, index in enumerate(df.set_index(id_col).index.unique())}\n",
    "\n",
    "    return mapping\n",
    "\n",
    "\n",
    "user_mappings = node_mappings(df_ratings, \"userId\")\n",
    "movie_mappings = node_mappings(df_ratings, \"movieId\")\n",
    "df_ratings[\"uid\"] = df_ratings[\"userId\"].apply(lambda x: user_mappings[x])\n",
    "df_ratings[\"mid\"] = df_ratings[\"movieId\"].apply(lambda x: movie_mappings[x])\n",
    "edge_index = torch.from_numpy(df_ratings[[\"uid\", \"mid\"]].astype(int).values.T).clone()\n",
    "edge_label = torch.from_numpy(\n",
    "    (df_ratings[\"rating\"].astype(float) * 2)\n",
    "    .astype(int)\n",
    "    .values  # 0.5単位なので、2倍して10段階評価に変更\n",
    ").clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b91a4bd6-7acb-4daf-a91c-1d9861128e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94983a7524bb4d6bb103666a79c67cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1421 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title Embeddings shape: torch.Size([45463, 384])\n",
      "Encoded Genres shape: torch.Size([45463, 20])\n",
      "Shape of the concatenated features: torch.Size([45463, 404])\n"
     ]
    }
   ],
   "source": [
    "# 映画タイトル畳み込み\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "title_embeddings = model.encode(\n",
    "    df_metadata[\"title\"].values,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_tensor=True,\n",
    "    device=device,\n",
    ")\n",
    "print(\"Title Embeddings shape:\", title_embeddings.shape)\n",
    "\n",
    "# ジャンルのエンコード\n",
    "def genre_list_to_genre_names(genre_list):\n",
    "    genre_list = ast.literal_eval(genre_list)\n",
    "    if len(genre_list) == 0:\n",
    "        return []\n",
    "    genre_names = []\n",
    "    for genre_dict in genre_list:\n",
    "        genre_names.append(genre_dict[\"name\"])\n",
    "    return genre_names\n",
    "\n",
    "\n",
    "genres = df_metadata[\"genres\"].apply(genre_list_to_genre_names).values\n",
    "unique_genres = set(itertools.chain(*genres))\n",
    "mapping = {g: i for i, g in enumerate(unique_genres)}\n",
    "encoded_genres = torch.zeros(len(genres), len(mapping))\n",
    "for i, genre in enumerate(genres):\n",
    "    for g in genre:\n",
    "        encoded_genres[i, mapping[g]] = 1\n",
    "encoded_genres = encoded_genres.to(device)\n",
    "print(\"Encoded Genres shape:\", encoded_genres.shape)\n",
    "\n",
    "# 映画の特徴量を結合\n",
    "movie_x = torch.cat((title_embeddings, encoded_genres), dim=-1)\n",
    "print(\"Shape of the concatenated features:\", movie_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "953e768b-938f-479a-839f-7bcea9b0fcae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1muser\u001b[0m={ x=[671, 671] },\n",
       "  \u001b[1mmovie\u001b[0m={ x=[45463, 404] },\n",
       "  \u001b[1m(user, rates, movie)\u001b[0m={\n",
       "    edge_index=[2, 100004],\n",
       "    edge_label=[100004]\n",
       "  },\n",
       "  \u001b[1m(movie, rev_rates, user)\u001b[0m={ edge_index=[2, 100004] }\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# グラフを作成\n",
    "data = HeteroData()\n",
    "l = len(set(df_ratings[\"userId\"].values))\n",
    "data[\"user\"].x = torch.eye(l)  # 必ずしもone-hotでなくても良い？\n",
    "data[\"movie\"].x = movie_x\n",
    "data[\"user\", \"rates\", \"movie\"].edge_index = edge_index\n",
    "data[\"user\", \"rates\", \"movie\"].edge_label = edge_label\n",
    "data = ToUndirected()(data)  # 映画->ユーザーのエッジも作成\n",
    "del data[\"movie\", \"rev_rates\", \"user\"].edge_label  # 映画->ユーザーのエッジラベルを削除\n",
    "data = data.to(device)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38289d1e-2942-4044-90b0-06357a6e132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# エッジを学習・検証・テストに分割\n",
    "train_data, val_data, test_data = T.RandomLinkSplit(\n",
    "    num_val=0.1,\n",
    "    num_test=0.1,\n",
    "    neg_sampling_ratio=0.0,\n",
    "    edge_types=[(\"user\", \"rates\", \"movie\")],\n",
    "    rev_edge_types=[(\"movie\", \"rev_rates\", \"user\")],\n",
    ")(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21000d26-053d-4821-8dc5-c104627647ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    inf, 25.9424,  8.5413, 17.0572,  3.9632,  6.4874,  1.4312,  2.7345,\n",
       "         1.0000,  3.6576,  1.9007], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# レートに偏りがあるため、重みづけする。\n",
    "def weighted_mse_loss(pred, target, weight=None):\n",
    "    weight = 1.0 if weight is None else weight[target].to(pred.dtype)\n",
    "    return (weight * (pred - target.to(pred.dtype)).pow(2)).mean()\n",
    "\n",
    "\n",
    "weight = torch.bincount(train_data[\"user\", \"movie\"].edge_label)\n",
    "weight = weight.max() / weight\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f842496-7238-4c16-9eb2-89e1942f7a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # these convolutions have been replicated to match the number of edge types\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv2 = SAGEConv((-1, -1), out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EdgeDecoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.lin1 = Linear(2 * hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, z_dict, edge_label_index):\n",
    "        row, col = edge_label_index\n",
    "        # concat user and movie embeddings\n",
    "        z = torch.cat([z_dict[\"user\"][row], z_dict[\"movie\"][col]], dim=-1)\n",
    "        # concatenated embeddings passed to linear layer\n",
    "        z = self.lin1(z).relu()\n",
    "        z = self.lin2(z)\n",
    "        return z.view(-1)\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.encoder = GNNEncoder(hidden_channels, hidden_channels)\n",
    "        self.encoder = to_hetero(self.encoder, data.metadata(), aggr=\"sum\")\n",
    "        self.decoder = EdgeDecoder(hidden_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, edge_label_index):\n",
    "        # z_dict contains dictionary of movie and user embeddings returned from GraphSage\n",
    "        z_dict = self.encoder(x_dict, edge_index_dict)\n",
    "        return self.decoder(z_dict, edge_label_index)\n",
    "\n",
    "\n",
    "model = Model(hidden_channels=32).to(device)\n",
    "\n",
    "# Due to lazy initialization, we need to run one model step so the number\n",
    "# of parameters can be inferred:\n",
    "with torch.no_grad():\n",
    "    model.encoder(train_data.x_dict, train_data.edge_index_dict)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa1c53e9-5c46-409c-a0fb-3e2f9d9c6455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(\n",
    "        train_data.x_dict,\n",
    "        train_data.edge_index_dict,\n",
    "        train_data[\"user\", \"movie\"].edge_label_index,\n",
    "    )\n",
    "    target = train_data[\"user\", \"movie\"].edge_label\n",
    "    loss = weighted_mse_loss(pred, target, weight)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3c187bb-c1f3-4f7a-b27d-958e6b7c61ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    pred = model(\n",
    "        data.x_dict, data.edge_index_dict, data[\"user\", \"movie\"].edge_label_index\n",
    "    )\n",
    "    pred = pred.clamp(min=0, max=5)\n",
    "    target = data[\"user\", \"movie\"].edge_label.float()\n",
    "    rmse = F.mse_loss(pred, target).sqrt()\n",
    "    return float(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed15f74a-0c0f-4c5d-bdc1-348561d852fd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 111.2115, Train: 7.2996, Val: 7.2794, Test: 7.2958\n",
      "Epoch: 002, Loss: 107.2463, Train: 7.1517, Val: 7.1324, Test: 7.1486\n",
      "Epoch: 003, Loss: 102.5272, Train: 6.9386, Val: 6.9204, Test: 6.9361\n",
      "Epoch: 004, Loss: 95.9481, Train: 6.5703, Val: 6.5545, Test: 6.5694\n",
      "Epoch: 005, Loss: 85.2134, Train: 5.9707, Val: 5.9588, Test: 5.9724\n",
      "Epoch: 006, Loss: 69.4457, Train: 5.0174, Val: 5.0122, Test: 5.0231\n",
      "Epoch: 007, Loss: 48.8176, Train: 3.6495, Val: 3.6536, Test: 3.6594\n",
      "Epoch: 008, Loss: 29.1773, Train: 2.9752, Val: 2.9580, Test: 2.9654\n",
      "Epoch: 009, Loss: 26.2849, Train: 2.9752, Val: 2.9566, Test: 2.9643\n",
      "Epoch: 010, Loss: 46.5691, Train: 2.9752, Val: 2.9566, Test: 2.9643\n",
      "Epoch: 011, Loss: 42.8651, Train: 2.9752, Val: 2.9567, Test: 2.9644\n",
      "Epoch: 012, Loss: 30.1798, Train: 2.9754, Val: 2.9633, Test: 2.9703\n",
      "Epoch: 013, Loss: 23.7193, Train: 3.1062, Val: 3.1116, Test: 3.1150\n",
      "Epoch: 014, Loss: 24.6253, Train: 3.5831, Val: 3.5884, Test: 3.5939\n",
      "Epoch: 015, Loss: 28.3480, Train: 3.8750, Val: 3.8784, Test: 3.8853\n",
      "Epoch: 016, Loss: 31.3842, Train: 3.9800, Val: 3.9828, Test: 3.9900\n",
      "Epoch: 017, Loss: 32.6113, Train: 3.9242, Val: 3.9274, Test: 3.9345\n",
      "Epoch: 018, Loss: 31.9137, Train: 3.7300, Val: 3.7348, Test: 3.7410\n",
      "Epoch: 019, Loss: 29.6985, Train: 3.4199, Val: 3.4270, Test: 3.4318\n",
      "Epoch: 020, Loss: 26.7089, Train: 3.0734, Val: 3.0792, Test: 3.0824\n",
      "Epoch: 021, Loss: 23.9798, Train: 2.9772, Val: 2.9703, Test: 2.9767\n",
      "Epoch: 022, Loss: 22.6454, Train: 2.9752, Val: 2.9592, Test: 2.9670\n",
      "Epoch: 023, Loss: 23.3291, Train: 2.9752, Val: 2.9571, Test: 2.9648\n",
      "Epoch: 024, Loss: 25.2870, Train: 2.9752, Val: 2.9568, Test: 2.9645\n",
      "Epoch: 025, Loss: 26.5602, Train: 2.9752, Val: 2.9569, Test: 2.9646\n",
      "Epoch: 026, Loss: 25.8970, Train: 2.9752, Val: 2.9575, Test: 2.9654\n",
      "Epoch: 027, Loss: 24.0107, Train: 2.9752, Val: 2.9604, Test: 2.9681\n",
      "Epoch: 028, Loss: 22.4322, Train: 2.9764, Val: 2.9689, Test: 2.9756\n",
      "Epoch: 029, Loss: 21.9676, Train: 2.9964, Val: 2.9968, Test: 3.0021\n",
      "Epoch: 030, Loss: 22.4188, Train: 3.0570, Val: 3.0621, Test: 3.0667\n",
      "Epoch: 031, Loss: 23.1421, Train: 3.1098, Val: 3.1168, Test: 3.1213\n",
      "Epoch: 032, Loss: 23.5785, Train: 3.1059, Val: 3.1127, Test: 3.1174\n",
      "Epoch: 033, Loss: 23.4683, Train: 3.0540, Val: 3.0586, Test: 3.0635\n",
      "Epoch: 034, Loss: 22.8564, Train: 3.0011, Val: 3.0015, Test: 3.0073\n",
      "Epoch: 035, Loss: 22.0168, Train: 2.9788, Val: 2.9737, Test: 2.9803\n",
      "Epoch: 036, Loss: 21.3283, Train: 2.9749, Val: 2.9638, Test: 2.9713\n",
      "Epoch: 037, Loss: 21.0878, Train: 2.9747, Val: 2.9598, Test: 2.9679\n",
      "Epoch: 038, Loss: 21.3034, Train: 2.9748, Val: 2.9583, Test: 2.9665\n",
      "Epoch: 039, Loss: 21.6398, Train: 2.9748, Val: 2.9579, Test: 2.9662\n",
      "Epoch: 040, Loss: 21.6749, Train: 2.9747, Val: 2.9585, Test: 2.9667\n",
      "Epoch: 041, Loss: 21.2845, Train: 2.9744, Val: 2.9599, Test: 2.9682\n",
      "Epoch: 042, Loss: 20.7202, Train: 2.9747, Val: 2.9632, Test: 2.9713\n",
      "Epoch: 043, Loss: 20.3185, Train: 2.9774, Val: 2.9694, Test: 2.9771\n",
      "Epoch: 044, Loss: 20.2072, Train: 2.9843, Val: 2.9788, Test: 2.9860\n",
      "Epoch: 045, Loss: 20.2758, Train: 2.9921, Val: 2.9878, Test: 2.9947\n",
      "Epoch: 046, Loss: 20.3214, Train: 2.9943, Val: 2.9898, Test: 2.9968\n",
      "Epoch: 047, Loss: 20.2024, Train: 2.9896, Val: 2.9839, Test: 2.9912\n",
      "Epoch: 048, Loss: 19.9082, Train: 2.9827, Val: 2.9750, Test: 2.9830\n",
      "Epoch: 049, Loss: 19.5441, Train: 2.9776, Val: 2.9679, Test: 2.9763\n",
      "Epoch: 050, Loss: 19.2539, Train: 2.9752, Val: 2.9636, Test: 2.9723\n",
      "Epoch: 051, Loss: 19.1158, Train: 2.9745, Val: 2.9614, Test: 2.9704\n",
      "Epoch: 052, Loss: 19.0783, Train: 2.9743, Val: 2.9607, Test: 2.9698\n",
      "Epoch: 053, Loss: 19.0036, Train: 2.9745, Val: 2.9612, Test: 2.9705\n",
      "Epoch: 054, Loss: 18.7993, Train: 2.9757, Val: 2.9633, Test: 2.9725\n",
      "Epoch: 055, Loss: 18.5050, Train: 2.9788, Val: 2.9674, Test: 2.9767\n",
      "Epoch: 056, Loss: 18.2373, Train: 2.9849, Val: 2.9744, Test: 2.9835\n",
      "Epoch: 057, Loss: 18.0646, Train: 2.9936, Val: 2.9842, Test: 2.9925\n",
      "Epoch: 058, Loss: 17.9894, Train: 2.9995, Val: 2.9897, Test: 2.9981\n",
      "Epoch: 059, Loss: 17.8447, Train: 3.0019, Val: 2.9918, Test: 3.0003\n",
      "Epoch: 060, Loss: 17.6667, Train: 3.0000, Val: 2.9892, Test: 2.9981\n",
      "Epoch: 061, Loss: 17.4424, Train: 2.9959, Val: 2.9845, Test: 2.9939\n",
      "Epoch: 062, Loss: 17.2323, Train: 2.9937, Val: 2.9817, Test: 2.9915\n",
      "Epoch: 063, Loss: 17.0844, Train: 2.9931, Val: 2.9806, Test: 2.9907\n",
      "Epoch: 064, Loss: 16.9644, Train: 2.9959, Val: 2.9831, Test: 2.9934\n",
      "Epoch: 065, Loss: 16.8410, Train: 3.0040, Val: 2.9913, Test: 3.0016\n",
      "Epoch: 066, Loss: 16.7018, Train: 3.0173, Val: 3.0049, Test: 3.0149\n",
      "Epoch: 067, Loss: 16.5976, Train: 3.0248, Val: 3.0123, Test: 3.0223\n",
      "Epoch: 068, Loss: 16.5034, Train: 3.0252, Val: 3.0122, Test: 3.0226\n",
      "Epoch: 069, Loss: 16.4120, Train: 3.0260, Val: 3.0126, Test: 3.0233\n",
      "Epoch: 070, Loss: 16.3590, Train: 3.0328, Val: 3.0193, Test: 3.0301\n",
      "Epoch: 071, Loss: 16.3141, Train: 3.0456, Val: 3.0322, Test: 3.0430\n",
      "Epoch: 072, Loss: 16.2747, Train: 3.0589, Val: 3.0456, Test: 3.0565\n",
      "Epoch: 073, Loss: 16.2599, Train: 3.0640, Val: 3.0508, Test: 3.0618\n",
      "Epoch: 074, Loss: 16.2425, Train: 3.0621, Val: 3.0487, Test: 3.0599\n",
      "Epoch: 075, Loss: 16.2206, Train: 3.0597, Val: 3.0462, Test: 3.0576\n",
      "Epoch: 076, Loss: 16.2041, Train: 3.0609, Val: 3.0476, Test: 3.0591\n",
      "Epoch: 077, Loss: 16.1774, Train: 3.0651, Val: 3.0520, Test: 3.0636\n",
      "Epoch: 078, Loss: 16.1407, Train: 3.0685, Val: 3.0558, Test: 3.0675\n",
      "Epoch: 079, Loss: 16.1046, Train: 3.0675, Val: 3.0550, Test: 3.0668\n",
      "Epoch: 080, Loss: 16.0648, Train: 3.0620, Val: 3.0497, Test: 3.0614\n",
      "Epoch: 081, Loss: 16.0198, Train: 3.0553, Val: 3.0432, Test: 3.0549\n",
      "Epoch: 082, Loss: 15.9780, Train: 3.0507, Val: 3.0389, Test: 3.0506\n",
      "Epoch: 083, Loss: 15.9378, Train: 3.0487, Val: 3.0375, Test: 3.0491\n",
      "Epoch: 084, Loss: 15.8960, Train: 3.0477, Val: 3.0371, Test: 3.0486\n",
      "Epoch: 085, Loss: 15.8582, Train: 3.0451, Val: 3.0351, Test: 3.0466\n",
      "Epoch: 086, Loss: 15.8242, Train: 3.0399, Val: 3.0305, Test: 3.0418\n",
      "Epoch: 087, Loss: 15.7905, Train: 3.0338, Val: 3.0249, Test: 3.0361\n",
      "Epoch: 088, Loss: 15.7609, Train: 3.0295, Val: 3.0212, Test: 3.0323\n",
      "Epoch: 089, Loss: 15.7345, Train: 3.0279, Val: 3.0204, Test: 3.0313\n",
      "Epoch: 090, Loss: 15.7054, Train: 3.0275, Val: 3.0211, Test: 3.0317\n",
      "Epoch: 091, Loss: 15.6757, Train: 3.0268, Val: 3.0214, Test: 3.0317\n",
      "Epoch: 092, Loss: 15.6442, Train: 3.0256, Val: 3.0210, Test: 3.0312\n",
      "Epoch: 093, Loss: 15.6088, Train: 3.0253, Val: 3.0214, Test: 3.0316\n",
      "Epoch: 094, Loss: 15.5739, Train: 3.0273, Val: 3.0242, Test: 3.0344\n",
      "Epoch: 095, Loss: 15.5374, Train: 3.0309, Val: 3.0289, Test: 3.0387\n",
      "Epoch: 096, Loss: 15.5016, Train: 3.0342, Val: 3.0331, Test: 3.0427\n",
      "Epoch: 097, Loss: 15.4692, Train: 3.0364, Val: 3.0361, Test: 3.0455\n",
      "Epoch: 098, Loss: 15.4380, Train: 3.0379, Val: 3.0381, Test: 3.0474\n",
      "Epoch: 099, Loss: 15.4068, Train: 3.0397, Val: 3.0406, Test: 3.0496\n",
      "Epoch: 100, Loss: 15.3724, Train: 3.0413, Val: 3.0431, Test: 3.0519\n",
      "Epoch: 101, Loss: 15.3337, Train: 3.0416, Val: 3.0440, Test: 3.0525\n",
      "Epoch: 102, Loss: 15.2908, Train: 3.0402, Val: 3.0430, Test: 3.0514\n",
      "Epoch: 103, Loss: 15.2451, Train: 3.0388, Val: 3.0419, Test: 3.0504\n",
      "Epoch: 104, Loss: 15.1994, Train: 3.0385, Val: 3.0424, Test: 3.0506\n",
      "Epoch: 105, Loss: 15.1541, Train: 3.0384, Val: 3.0428, Test: 3.0508\n",
      "Epoch: 106, Loss: 15.1094, Train: 3.0377, Val: 3.0425, Test: 3.0503\n",
      "Epoch: 107, Loss: 15.0650, Train: 3.0375, Val: 3.0427, Test: 3.0503\n",
      "Epoch: 108, Loss: 15.0208, Train: 3.0388, Val: 3.0447, Test: 3.0520\n",
      "Epoch: 109, Loss: 14.9767, Train: 3.0392, Val: 3.0455, Test: 3.0527\n",
      "Epoch: 110, Loss: 14.9317, Train: 3.0401, Val: 3.0470, Test: 3.0540\n",
      "Epoch: 111, Loss: 14.8861, Train: 3.0423, Val: 3.0502, Test: 3.0568\n",
      "Epoch: 112, Loss: 14.8397, Train: 3.0423, Val: 3.0507, Test: 3.0573\n",
      "Epoch: 113, Loss: 14.7927, Train: 3.0437, Val: 3.0529, Test: 3.0594\n",
      "Epoch: 114, Loss: 14.7453, Train: 3.0455, Val: 3.0557, Test: 3.0619\n",
      "Epoch: 115, Loss: 14.6979, Train: 3.0447, Val: 3.0555, Test: 3.0616\n",
      "Epoch: 116, Loss: 14.6505, Train: 3.0473, Val: 3.0595, Test: 3.0653\n",
      "Epoch: 117, Loss: 14.6033, Train: 3.0444, Val: 3.0566, Test: 3.0626\n",
      "Epoch: 118, Loss: 14.5572, Train: 3.0499, Val: 3.0640, Test: 3.0694\n",
      "Epoch: 119, Loss: 14.5133, Train: 3.0413, Val: 3.0538, Test: 3.0600\n",
      "Epoch: 120, Loss: 14.4744, Train: 3.0549, Val: 3.0711, Test: 3.0761\n",
      "Epoch: 121, Loss: 14.4408, Train: 3.0396, Val: 3.0523, Test: 3.0588\n",
      "Epoch: 122, Loss: 14.3920, Train: 3.0481, Val: 3.0629, Test: 3.0688\n",
      "Epoch: 123, Loss: 14.3319, Train: 3.0503, Val: 3.0656, Test: 3.0716\n",
      "Epoch: 124, Loss: 14.2954, Train: 3.0393, Val: 3.0519, Test: 3.0589\n",
      "Epoch: 125, Loss: 14.2652, Train: 3.0500, Val: 3.0649, Test: 3.0714\n",
      "Epoch: 126, Loss: 14.2177, Train: 3.0471, Val: 3.0612, Test: 3.0680\n",
      "Epoch: 127, Loss: 14.1764, Train: 3.0397, Val: 3.0520, Test: 3.0595\n",
      "Epoch: 128, Loss: 14.1520, Train: 3.0500, Val: 3.0645, Test: 3.0715\n",
      "Epoch: 129, Loss: 14.1165, Train: 3.0450, Val: 3.0584, Test: 3.0659\n",
      "Epoch: 130, Loss: 14.0783, Train: 3.0398, Val: 3.0521, Test: 3.0599\n",
      "Epoch: 131, Loss: 14.0572, Train: 3.0492, Val: 3.0634, Test: 3.0708\n",
      "Epoch: 132, Loss: 14.0307, Train: 3.0431, Val: 3.0560, Test: 3.0638\n",
      "Epoch: 133, Loss: 13.9972, Train: 3.0397, Val: 3.0520, Test: 3.0600\n",
      "Epoch: 134, Loss: 13.9773, Train: 3.0481, Val: 3.0621, Test: 3.0696\n",
      "Epoch: 135, Loss: 13.9569, Train: 3.0411, Val: 3.0535, Test: 3.0615\n",
      "Epoch: 136, Loss: 13.9279, Train: 3.0407, Val: 3.0531, Test: 3.0610\n",
      "Epoch: 137, Loss: 13.9067, Train: 3.0470, Val: 3.0606, Test: 3.0681\n",
      "Epoch: 138, Loss: 13.8905, Train: 3.0400, Val: 3.0520, Test: 3.0600\n",
      "Epoch: 139, Loss: 13.8674, Train: 3.0427, Val: 3.0550, Test: 3.0629\n",
      "Epoch: 140, Loss: 13.8455, Train: 3.0461, Val: 3.0589, Test: 3.0666\n",
      "Epoch: 141, Loss: 13.8298, Train: 3.0400, Val: 3.0513, Test: 3.0595\n",
      "Epoch: 142, Loss: 13.8133, Train: 3.0456, Val: 3.0579, Test: 3.0657\n",
      "Epoch: 143, Loss: 13.7930, Train: 3.0446, Val: 3.0565, Test: 3.0645\n",
      "Epoch: 144, Loss: 13.7749, Train: 3.0410, Val: 3.0520, Test: 3.0601\n",
      "Epoch: 145, Loss: 13.7607, Train: 3.0472, Val: 3.0592, Test: 3.0671\n",
      "Epoch: 146, Loss: 13.7448, Train: 3.0423, Val: 3.0532, Test: 3.0614\n",
      "Epoch: 147, Loss: 13.7260, Train: 3.0434, Val: 3.0544, Test: 3.0625\n",
      "Epoch: 148, Loss: 13.7092, Train: 3.0462, Val: 3.0576, Test: 3.0655\n",
      "Epoch: 149, Loss: 13.6953, Train: 3.0409, Val: 3.0511, Test: 3.0593\n",
      "Epoch: 150, Loss: 13.6810, Train: 3.0457, Val: 3.0567, Test: 3.0647\n",
      "Epoch: 151, Loss: 13.6647, Train: 3.0427, Val: 3.0530, Test: 3.0611\n",
      "Epoch: 152, Loss: 13.6485, Train: 3.0420, Val: 3.0522, Test: 3.0603\n",
      "Epoch: 153, Loss: 13.6342, Train: 3.0451, Val: 3.0558, Test: 3.0637\n",
      "Epoch: 154, Loss: 13.6212, Train: 3.0399, Val: 3.0495, Test: 3.0577\n",
      "Epoch: 155, Loss: 13.6082, Train: 3.0449, Val: 3.0554, Test: 3.0634\n",
      "Epoch: 156, Loss: 13.5939, Train: 3.0404, Val: 3.0500, Test: 3.0581\n",
      "Epoch: 157, Loss: 13.5793, Train: 3.0430, Val: 3.0530, Test: 3.0610\n",
      "Epoch: 158, Loss: 13.5648, Train: 3.0422, Val: 3.0519, Test: 3.0600\n",
      "Epoch: 159, Loss: 13.5509, Train: 3.0408, Val: 3.0502, Test: 3.0584\n",
      "Epoch: 160, Loss: 13.5378, Train: 3.0441, Val: 3.0541, Test: 3.0622\n",
      "Epoch: 161, Loss: 13.5258, Train: 3.0381, Val: 3.0469, Test: 3.0552\n",
      "Epoch: 162, Loss: 13.5158, Train: 3.0479, Val: 3.0583, Test: 3.0664\n",
      "Epoch: 163, Loss: 13.5099, Train: 3.0331, Val: 3.0409, Test: 3.0494\n",
      "Epoch: 164, Loss: 13.5135, Train: 3.0546, Val: 3.0660, Test: 3.0739\n",
      "Epoch: 165, Loss: 13.5211, Train: 3.0294, Val: 3.0363, Test: 3.0450\n",
      "Epoch: 166, Loss: 13.5178, Train: 3.0505, Val: 3.0612, Test: 3.0694\n",
      "Epoch: 167, Loss: 13.4723, Train: 3.0396, Val: 3.0484, Test: 3.0570\n",
      "Epoch: 168, Loss: 13.4345, Train: 3.0351, Val: 3.0431, Test: 3.0518\n",
      "Epoch: 169, Loss: 13.4331, Train: 3.0507, Val: 3.0613, Test: 3.0696\n",
      "Epoch: 170, Loss: 13.4392, Train: 3.0323, Val: 3.0398, Test: 3.0486\n",
      "Epoch: 171, Loss: 13.4203, Train: 3.0431, Val: 3.0525, Test: 3.0612\n",
      "Epoch: 172, Loss: 13.3867, Train: 3.0431, Val: 3.0524, Test: 3.0612\n",
      "Epoch: 173, Loss: 13.3745, Train: 3.0324, Val: 3.0399, Test: 3.0488\n",
      "Epoch: 174, Loss: 13.3776, Train: 3.0477, Val: 3.0576, Test: 3.0664\n",
      "Epoch: 175, Loss: 13.3662, Train: 3.0348, Val: 3.0427, Test: 3.0518\n",
      "Epoch: 176, Loss: 13.3397, Train: 3.0386, Val: 3.0471, Test: 3.0562\n",
      "Epoch: 177, Loss: 13.3200, Train: 3.0440, Val: 3.0534, Test: 3.0624\n",
      "Epoch: 178, Loss: 13.3150, Train: 3.0321, Val: 3.0395, Test: 3.0487\n",
      "Epoch: 179, Loss: 13.3118, Train: 3.0464, Val: 3.0560, Test: 3.0651\n",
      "Epoch: 180, Loss: 13.2977, Train: 3.0341, Val: 3.0419, Test: 3.0512\n",
      "Epoch: 181, Loss: 13.2757, Train: 3.0403, Val: 3.0491, Test: 3.0584\n",
      "Epoch: 182, Loss: 13.2548, Train: 3.0408, Val: 3.0497, Test: 3.0591\n",
      "Epoch: 183, Loss: 13.2419, Train: 3.0339, Val: 3.0417, Test: 3.0511\n",
      "Epoch: 184, Loss: 13.2351, Train: 3.0457, Val: 3.0552, Test: 3.0647\n",
      "Epoch: 185, Loss: 13.2287, Train: 3.0310, Val: 3.0383, Test: 3.0479\n",
      "Epoch: 186, Loss: 13.2201, Train: 3.0465, Val: 3.0563, Test: 3.0658\n",
      "Epoch: 187, Loss: 13.2061, Train: 3.0310, Val: 3.0384, Test: 3.0480\n",
      "Epoch: 188, Loss: 13.1898, Train: 3.0444, Val: 3.0540, Test: 3.0636\n",
      "Epoch: 189, Loss: 13.1701, Train: 3.0328, Val: 3.0406, Test: 3.0502\n",
      "Epoch: 190, Loss: 13.1507, Train: 3.0412, Val: 3.0505, Test: 3.0600\n",
      "Epoch: 191, Loss: 13.1317, Train: 3.0346, Val: 3.0429, Test: 3.0525\n",
      "Epoch: 192, Loss: 13.1147, Train: 3.0390, Val: 3.0480, Test: 3.0576\n",
      "Epoch: 193, Loss: 13.0984, Train: 3.0352, Val: 3.0437, Test: 3.0533\n",
      "Epoch: 194, Loss: 13.0824, Train: 3.0378, Val: 3.0467, Test: 3.0563\n",
      "Epoch: 195, Loss: 13.0667, Train: 3.0349, Val: 3.0434, Test: 3.0531\n",
      "Epoch: 196, Loss: 13.0508, Train: 3.0385, Val: 3.0475, Test: 3.0571\n",
      "Epoch: 197, Loss: 13.0353, Train: 3.0326, Val: 3.0407, Test: 3.0503\n",
      "Epoch: 198, Loss: 13.0222, Train: 3.0434, Val: 3.0531, Test: 3.0627\n",
      "Epoch: 199, Loss: 13.0178, Train: 3.0233, Val: 3.0298, Test: 3.0394\n",
      "Epoch: 200, Loss: 13.0509, Train: 3.0661, Val: 3.0784, Test: 3.0881\n",
      "Epoch: 201, Loss: 13.1866, Train: 3.0059, Val: 3.0078, Test: 3.0177\n",
      "Epoch: 202, Loss: 13.4901, Train: 3.0777, Val: 3.0912, Test: 3.1010\n",
      "Epoch: 203, Loss: 13.3067, Train: 3.0268, Val: 3.0343, Test: 3.0437\n",
      "Epoch: 204, Loss: 12.9607, Train: 3.0203, Val: 3.0263, Test: 3.0358\n",
      "Epoch: 205, Loss: 13.0261, Train: 3.0673, Val: 3.0803, Test: 3.0898\n",
      "Epoch: 206, Loss: 13.1279, Train: 3.0252, Val: 3.0325, Test: 3.0417\n",
      "Epoch: 207, Loss: 12.9333, Train: 3.0241, Val: 3.0312, Test: 3.0404\n",
      "Epoch: 208, Loss: 12.9221, Train: 3.0608, Val: 3.0733, Test: 3.0825\n",
      "Epoch: 209, Loss: 13.0180, Train: 3.0229, Val: 3.0300, Test: 3.0392\n",
      "Epoch: 210, Loss: 12.8784, Train: 3.0244, Val: 3.0320, Test: 3.0411\n",
      "Epoch: 211, Loss: 12.8394, Train: 3.0543, Val: 3.0662, Test: 3.0753\n",
      "Epoch: 212, Loss: 12.9245, Train: 3.0195, Val: 3.0262, Test: 3.0354\n",
      "Epoch: 213, Loss: 12.8363, Train: 3.0266, Val: 3.0348, Test: 3.0437\n",
      "Epoch: 214, Loss: 12.7687, Train: 3.0498, Val: 3.0613, Test: 3.0701\n",
      "Epoch: 215, Loss: 12.8394, Train: 3.0178, Val: 3.0243, Test: 3.0333\n",
      "Epoch: 216, Loss: 12.8008, Train: 3.0318, Val: 3.0409, Test: 3.0497\n",
      "Epoch: 217, Loss: 12.7079, Train: 3.0451, Val: 3.0560, Test: 3.0647\n",
      "Epoch: 218, Loss: 12.7421, Train: 3.0182, Val: 3.0248, Test: 3.0335\n",
      "Epoch: 219, Loss: 12.7470, Train: 3.0374, Val: 3.0474, Test: 3.0559\n",
      "Epoch: 220, Loss: 12.6603, Train: 3.0400, Val: 3.0504, Test: 3.0589\n",
      "Epoch: 221, Loss: 12.6506, Train: 3.0201, Val: 3.0272, Test: 3.0356\n",
      "Epoch: 222, Loss: 12.6772, Train: 3.0420, Val: 3.0529, Test: 3.0611\n",
      "Epoch: 223, Loss: 12.6208, Train: 3.0339, Val: 3.0438, Test: 3.0519\n",
      "Epoch: 224, Loss: 12.5747, Train: 3.0225, Val: 3.0304, Test: 3.0386\n",
      "Epoch: 225, Loss: 12.5896, Train: 3.0438, Val: 3.0553, Test: 3.0633\n",
      "Epoch: 226, Loss: 12.5791, Train: 3.0245, Val: 3.0332, Test: 3.0413\n",
      "Epoch: 227, Loss: 12.5284, Train: 3.0291, Val: 3.0387, Test: 3.0467\n",
      "Epoch: 228, Loss: 12.4939, Train: 3.0381, Val: 3.0493, Test: 3.0571\n",
      "Epoch: 229, Loss: 12.4945, Train: 3.0199, Val: 3.0279, Test: 3.0360\n",
      "Epoch: 230, Loss: 12.4962, Train: 3.0400, Val: 3.0517, Test: 3.0593\n",
      "Epoch: 231, Loss: 12.4671, Train: 3.0241, Val: 3.0332, Test: 3.0410\n",
      "Epoch: 232, Loss: 12.4257, Train: 3.0298, Val: 3.0401, Test: 3.0476\n",
      "Epoch: 233, Loss: 12.3941, Train: 3.0343, Val: 3.0455, Test: 3.0529\n",
      "Epoch: 234, Loss: 12.3822, Train: 3.0217, Val: 3.0305, Test: 3.0381\n",
      "Epoch: 235, Loss: 12.3800, Train: 3.0404, Val: 3.0525, Test: 3.0597\n",
      "Epoch: 236, Loss: 12.3701, Train: 3.0203, Val: 3.0286, Test: 3.0363\n",
      "Epoch: 237, Loss: 12.3524, Train: 3.0386, Val: 3.0505, Test: 3.0577\n",
      "Epoch: 238, Loss: 12.3214, Train: 3.0233, Val: 3.0324, Test: 3.0400\n",
      "Epoch: 239, Loss: 12.2908, Train: 3.0336, Val: 3.0448, Test: 3.0520\n",
      "Epoch: 240, Loss: 12.2617, Train: 3.0276, Val: 3.0377, Test: 3.0450\n",
      "Epoch: 241, Loss: 12.2374, Train: 3.0294, Val: 3.0399, Test: 3.0471\n",
      "Epoch: 242, Loss: 12.2161, Train: 3.0308, Val: 3.0416, Test: 3.0488\n",
      "Epoch: 243, Loss: 12.1971, Train: 3.0251, Val: 3.0347, Test: 3.0422\n",
      "Epoch: 244, Loss: 12.1820, Train: 3.0366, Val: 3.0485, Test: 3.0556\n",
      "Epoch: 245, Loss: 12.1768, Train: 3.0169, Val: 3.0246, Test: 3.0325\n",
      "Epoch: 246, Loss: 12.1979, Train: 3.0542, Val: 3.0686, Test: 3.0754\n",
      "Epoch: 247, Loss: 12.2805, Train: 3.0021, Val: 3.0050, Test: 3.0140\n",
      "Epoch: 248, Loss: 12.5200, Train: 3.0825, Val: 3.0993, Test: 3.1057\n",
      "Epoch: 249, Loss: 12.6150, Train: 2.9995, Val: 3.0009, Test: 3.0104\n",
      "Epoch: 250, Loss: 12.6591, Train: 3.0441, Val: 3.0566, Test: 3.0638\n",
      "Epoch: 251, Loss: 12.1209, Train: 3.0576, Val: 3.0721, Test: 3.0789\n",
      "Epoch: 252, Loss: 12.2214, Train: 3.0022, Val: 3.0039, Test: 3.0132\n",
      "Epoch: 253, Loss: 12.5577, Train: 3.0476, Val: 3.0607, Test: 3.0675\n",
      "Epoch: 254, Loss: 12.0896, Train: 3.0564, Val: 3.0711, Test: 3.0774\n",
      "Epoch: 255, Loss: 12.1430, Train: 3.0012, Val: 3.0037, Test: 3.0126\n",
      "Epoch: 256, Loss: 12.4457, Train: 3.0464, Val: 3.0599, Test: 3.0666\n",
      "Epoch: 257, Loss: 12.0203, Train: 3.0504, Val: 3.0647, Test: 3.0713\n",
      "Epoch: 258, Loss: 12.0404, Train: 2.9996, Val: 3.0030, Test: 3.0115\n",
      "Epoch: 259, Loss: 12.3420, Train: 3.0477, Val: 3.0620, Test: 3.0686\n",
      "Epoch: 260, Loss: 11.9899, Train: 3.0414, Val: 3.0548, Test: 3.0616\n",
      "Epoch: 261, Loss: 11.9250, Train: 3.0040, Val: 3.0089, Test: 3.0171\n",
      "Epoch: 262, Loss: 12.1225, Train: 3.0450, Val: 3.0590, Test: 3.0656\n",
      "Epoch: 263, Loss: 11.9113, Train: 3.0380, Val: 3.0508, Test: 3.0577\n",
      "Epoch: 264, Loss: 11.8481, Train: 3.0089, Val: 3.0151, Test: 3.0232\n",
      "Epoch: 265, Loss: 11.9596, Train: 3.0412, Val: 3.0546, Test: 3.0616\n",
      "Epoch: 266, Loss: 11.8280, Train: 3.0357, Val: 3.0482, Test: 3.0555\n",
      "Epoch: 267, Loss: 11.7798, Train: 3.0110, Val: 3.0181, Test: 3.0264\n",
      "Epoch: 268, Loss: 11.8513, Train: 3.0408, Val: 3.0544, Test: 3.0616\n",
      "Epoch: 269, Loss: 11.7667, Train: 3.0295, Val: 3.0411, Test: 3.0487\n",
      "Epoch: 270, Loss: 11.7013, Train: 3.0143, Val: 3.0228, Test: 3.0309\n",
      "Epoch: 271, Loss: 11.7401, Train: 3.0436, Val: 3.0578, Test: 3.0649\n",
      "Epoch: 272, Loss: 11.7289, Train: 3.0191, Val: 3.0287, Test: 3.0365\n",
      "Epoch: 273, Loss: 11.6610, Train: 3.0244, Val: 3.0351, Test: 3.0427\n",
      "Epoch: 274, Loss: 11.6221, Train: 3.0392, Val: 3.0526, Test: 3.0598\n",
      "Epoch: 275, Loss: 11.6391, Train: 3.0147, Val: 3.0231, Test: 3.0311\n",
      "Epoch: 276, Loss: 11.6433, Train: 3.0372, Val: 3.0501, Test: 3.0575\n",
      "Epoch: 277, Loss: 11.5883, Train: 3.0268, Val: 3.0378, Test: 3.0456\n",
      "Epoch: 278, Loss: 11.5449, Train: 3.0212, Val: 3.0312, Test: 3.0392\n",
      "Epoch: 279, Loss: 11.5401, Train: 3.0399, Val: 3.0534, Test: 3.0609\n",
      "Epoch: 280, Loss: 11.5469, Train: 3.0159, Val: 3.0248, Test: 3.0331\n",
      "Epoch: 281, Loss: 11.5372, Train: 3.0367, Val: 3.0499, Test: 3.0576\n",
      "Epoch: 282, Loss: 11.4957, Train: 3.0236, Val: 3.0344, Test: 3.0426\n",
      "Epoch: 283, Loss: 11.4569, Train: 3.0253, Val: 3.0365, Test: 3.0446\n",
      "Epoch: 284, Loss: 11.4362, Train: 3.0342, Val: 3.0472, Test: 3.0550\n",
      "Epoch: 285, Loss: 11.4310, Train: 3.0176, Val: 3.0271, Test: 3.0357\n",
      "Epoch: 286, Loss: 11.4369, Train: 3.0430, Val: 3.0573, Test: 3.0649\n",
      "Epoch: 287, Loss: 11.4412, Train: 3.0130, Val: 3.0212, Test: 3.0301\n",
      "Epoch: 288, Loss: 11.4550, Train: 3.0470, Val: 3.0617, Test: 3.0694\n",
      "Epoch: 289, Loss: 11.4360, Train: 3.0132, Val: 3.0214, Test: 3.0305\n",
      "Epoch: 290, Loss: 11.4227, Train: 3.0428, Val: 3.0570, Test: 3.0649\n",
      "Epoch: 291, Loss: 11.3719, Train: 3.0200, Val: 3.0300, Test: 3.0389\n",
      "Epoch: 292, Loss: 11.3295, Train: 3.0327, Val: 3.0456, Test: 3.0538\n",
      "Epoch: 293, Loss: 11.2952, Train: 3.0291, Val: 3.0413, Test: 3.0498\n",
      "Epoch: 294, Loss: 11.2746, Train: 3.0235, Val: 3.0347, Test: 3.0434\n",
      "Epoch: 295, Loss: 11.2665, Train: 3.0391, Val: 3.0533, Test: 3.0615\n",
      "Epoch: 296, Loss: 11.2737, Train: 3.0135, Val: 3.0226, Test: 3.0318\n",
      "Epoch: 297, Loss: 11.3108, Train: 3.0585, Val: 3.0752, Test: 3.0830\n",
      "Epoch: 298, Loss: 11.3981, Train: 2.9999, Val: 3.0045, Test: 3.0150\n",
      "Epoch: 299, Loss: 11.6228, Train: 3.0879, Val: 3.1067, Test: 3.1145\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 300):\n",
    "    loss = train()\n",
    "    train_rmse = test(train_data)\n",
    "    val_rmse = test(val_data)\n",
    "    test_rmse = test(test_data)\n",
    "    print(\n",
    "        f\"Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_rmse:.4f}, \"\n",
    "        f\"Val: {val_rmse:.4f}, Test: {test_rmse:.4f}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
